---
title: "Machine Learning Applied to Hepatocellular Carcinoma"
author: "Sangil Lee, Yujing Lu, Josh Tomiyama"
date: "`r Sys.Date()`"
output: 
  beamer_presentation:
    theme: "Darmstadt"
    colortheme: "crane"
    incremental: true
    slide_level: 2
bibliography: references.bib
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
  
```{r loadPackages, echo = FALSE, message=FALSE, warning=FALSE}
suppressPackageStartupMessages(library(tidyr))
suppressPackageStartupMessages(library(knitr))
suppressPackageStartupMessages(library(dplyr))
suppressPackageStartupMessages(library(MachineShop))
suppressPackageStartupMessages(library(recipes))
suppressPackageStartupMessages(library(kableExtra))
suppressPackageStartupMessages(library(arsenal))
if(!dir.exists("./vi_cache/")){
  dir.create("./vi_cache/")
}
```  

```{r getData, echo = FALSE, cache=TRUE}
dat <- readRDS("./data/hcc_data.RDS")
# dat <- readRDS("~/Desktop/R practice/Machine learning class spring 2022/hcc-survival/hcc_data.RDS")
rec_base <- recipe(
  status ~ .,
  data = dat
) %>%
 role_case(stratum = status) %>%
  check_missing(status) %>%
  step_num2factor(status, transform = function(x) x + 1,
                  levels = c("died", "survive")) %>%
  step_num2factor(gender, transform = function(x) x + 1,
                  levels = c("female", "male")) %>%
  step_num2factor(symptom, transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(alc, transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(hepBsurfAnti,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(hepBeAnti,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(hepBcorAnti,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(hepCvirAnti,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(cirr,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(endemicCountries,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(smoke,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(diabetes,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(obese,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(hemochro,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(artHyper,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(chronRenal,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(hiv,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(Nasteato,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(esophVarices,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(spleno,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(portalHyper,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(portalVeinThromb,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(LiverMeta,transform = function(x) x + 1,
                  levels = c("no", 'yes')) %>%
  step_num2factor(RadioHallmark,transform = function(x) x + 1,
                  levels = c("no", 'yes'))

# %>%
#   step_num2factor(numNodules,transform = function(x) x + 1,
#                   levels = as.character(0:5))
dat2 <- bake(prep(rec_base), new_data = dat)
```

# Background HCC

## HepatoCellular Carcinoma (HCC)

* HepatoCellular Carcinoma (HCC) 6th most frequently diagnosed cancer. 

* Data mining approach to tailor evaluation and treatment for HCC are limited in the literature.

* Using the HCC dataset, we undertook the data mining approach to evaluate the patient level factors to identify those who are at risk of one year mortality. 

# Data Summary

\begin{itemize} 
    \item {Clinical data of 165 pts with HCC (demographic, risk factors, lab data, and survival features)}
    \item {49 features from HCC clinical practice guidelines (Table 1)}
    \item  About 80% male, 74% had alochol related liver disease, 27% had hepatitis B, 21 % had hepatitis C, and 90% had cirrhosis. 
    \item Missing data represents 10.22% of the whole dataset and only eight patients have complete information in all fields (4.85%).
    \item The target variable is the survival at 1 year, coded as 0 (dies) and 1 (lives). 
\end{itemize}

# Data Summary

```{r summary, results='asis', echo = FALSE}
cnm <- colnames(dat2)
tbl <- tableby(
  ~ gender + symptom + alc, data = dat2,
  control = tableby.control(
    cat.stats = c("countpct", "Nmiss"),
    numeric.stats = c("medianrange", "Nmiss"),
    digits = 2
  )
)
summary(tbl, title = 'Table 1. HCC Data Summary') 
```



# Random Forest Model

## Description

```{r, cache = TRUE}
fit_sl <- readRDS("./data/ModelCache/knn_corr_RForest_fit.RDS")
sfit_sl <- summary(as.MLModel(fit_sl))
hyperparam_sl <- Reduce(cbind, sfit_sl$TrainingStep1$params[sfit_sl$TrainingStep1$selected,])
```

* RF is a modification of bagging that is comparable to boosting but is simpler to train and tune- works by building and averaging a large collection of de-correlated trees.

* 10 fold CV

* Hyperparameter- TunedModel fnc with 

  -ntree = 500,

  -mtry = .(if (is.factor(y)) floor(sqrt(nvars)) else max(floor(nvars / 3), 1))

  -nodesize = .(if (is.factor(y)) 1 else 5))

* Applied and tuned KNN imputation and Correlation Filter


## Estimated Performance

```{r, cache = TRUE}
resamp_sl <- readRDS("./data/ModelCache/knn_corr_RForest_res.RDS")
sres_sl<- as.data.frame(summary(resamp_sl))
sres_sl_df <- as.data.frame(sres_sl) %>% 
  pivot_wider(names_from = Statistic, values_from = Value) 

sres_sl_df %>%
  kable(caption = "RF results with knn imputation and corr filter",
        digits = 3) %>%
  kable_styling(full_width = FALSE)
```

## Calibration Plots

```{r out.height="70%", out.width="70%", fig.align='center', results='hide'}
plot(calibration(resamp_sl), se = TRUE)
```

## Variable Importance

```{r warning=FALSE, out.width="80%", out.height="80%", fig.align='center', cache = TRUE}
set.seed(123123)
if(file.exists("./vi_cache/knn_corr_RForest_vi.RDS")){
  vi_sl <- readRDS("./vi_cache/knn_corr_RForest_vi.RDS")
}else{
  vi_sl <- varimp(fit_sl)
  saveRDS(vi_sl, "./vi_cache/knn_corr_RForest_vi.RDS")
}
plot(vi_sl, n=5)
```

## Partial Dependence

```{r results='hide'}
pd_sl <- dependence(fit_sl, select = c(alphaFreto))
plot(pd_sl)
```

# XGBoost Model

## Description I

{\small Input: training set $\{(x_i, y_i)\}_{i=1}^N$, a differentiable loss function $L(y, F(x))$, a number of weak learners $M$ and a learning rate $\alpha$.} \

Algorithm: \
\begin{enumerate}
    \item Initialize model with a constant value: $\hat{f}_{(0)}(x) = argmin_{\theta}\sum_{i=1}^N L(y_i, \theta)$.
    \item For $m = 1, ..., M$:
        \begin{itemize}
            \item[a] Compute gradients and hessians:
                $\hat{g}_m(x_i) = \left[\frac{\partial L(y_i, f(x_i))}{\partial f(x_i)}\right]_{f(x) = \hat{f}_{m-1}(x)}$ \\
                $\hat{h}_m(x_i) = \left[\frac{\partial^2 L(y_i, f(x_i))}{\partial f(x_i)^2}\right]_{f(x) = \hat{f}_{m-1}(x)}$.
            \item[b] Fit a base learner using the training set $\bigg\{x_i, -\frac{\hat{g}_m(x_i)}{\hat{h}_m(x_i)}\bigg\}_{i=1}^N$ by solving the optimization problem below: \\
                $\hat{\phi}_m = argmin_{\phi \in \Phi} \sum_{i=1}^N \frac{1}{2}\hat{h}_m(x_i)\left[-\frac{\hat{g}_m(x_i)}{\hat{h}_m(x_i)} - \phi(x_i)\right]^2$.
            \item[c] Update the model: \\
                $\hat{f}_m(x) = \hat{f}_{(m-1)}(x) + \alpha \hat{\phi}_m$
        \end{itemize}
    \item Output $\hat{f}(x) = \hat{f}_{(M)}(x)$.
\end{enumerate}

## Description II

* Tuning parameters: number of boosting iterations $M$, shrinkage of variable weights at each iteration to prevent overfitting $\eta$, maximum tree depth.

* put some advantages and disadvantages here

## Estimated Performance I

```{r, echo=FALSE}
res_knn_none <- readRDS("./data/ModelCache/knn_none_xgboost_res.RDS")
res_knn_corr <- readRDS("./data/ModelCache/knn_corr_xgboost_res.RDS")
res_knn_pca <- readRDS("./data/ModelCache/knn_pca_xgboost_res.RDS")
res_meanmode_none <- readRDS("./data/ModelCache/MeanMode_none_xgboost_res.RDS")
res_meanmode_corr <- readRDS("./data/ModelCache/MeanMode_corr_xgboost_res.RDS")
res_meanmode_pca <- readRDS("./data/ModelCache/MeanMode_pca_xgboost_res.RDS")

# res_knn_none <- readRDS("H:/BIOS_6720/Project_YL/knn_none_xgboost_res.RDS")
# res_knn_corr <- readRDS("H:/BIOS_6720/Project_YL/knn_corr_xgboost_res.RDS")
# res_knn_pca <- readRDS("H:/BIOS_6720/Project_YL/knn_pca_xgboost_res.RDS")
# res_meanmode_none <- readRDS("H:/BIOS_6720/Project_YL/MeanMode_none_xgboost_res.RDS")
# res_meanmode_corr <- readRDS("H:/BIOS_6720/Project_YL/MeanMode_corr_xgboost_res.RDS")
# res_meanmode_pca <- readRDS("H:/BIOS_6720/Project_YL/MeanMode_pca_xgboost_res.RDS")

sres_knn_none <- as.data.frame(summary(res_knn_none))
sres_knn_corr <- as.data.frame(summary(res_knn_corr))
sres_knn_pca <- as.data.frame(summary(res_knn_pca))
sres_meanmode_none <- as.data.frame(summary(res_meanmode_none))
sres_meanmode_corr <- as.data.frame(summary(res_meanmode_corr))
sres_meanmode_pca <- as.data.frame(summary(res_meanmode_pca))

sres_knn_none <- as.data.frame(sres_knn_none) %>%
  pivot_wider(names_from = Statistic, values_from = Value)
sres_knn_corr <- as.data.frame(sres_knn_corr) %>%
  pivot_wider(names_from = Statistic, values_from = Value)
sres_knn_pca <- as.data.frame(sres_knn_pca) %>%
  pivot_wider(names_from = Statistic, values_from = Value)
sres_meanmode_none <- as.data.frame(sres_meanmode_none) %>%
  pivot_wider(names_from = Statistic, values_from = Value)
sres_meanmode_corr <- as.data.frame(sres_meanmode_corr) %>%
  pivot_wider(names_from = Statistic, values_from = Value)
sres_meanmode_pca <- as.data.frame(sres_meanmode_pca) %>%
  pivot_wider(names_from = Statistic, values_from = Value)

xgboost_all_mean_tab <- cbind(sres_knn_none[, 1], sres_knn_none[, 2], sres_knn_corr[, 2], sres_knn_pca[, 2],
      sres_meanmode_none[, 2], sres_meanmode_corr[, 2], sres_meanmode_pca[, 2])

names(xgboost_all_mean_tab) <- c("Metric", "KNN_none", "KNN_corr", "KNN_pca", "MeanMode_none", "MeanMode_corr", "MeanMode_pca")

kable(xgboost_all_mean_tab)
```

```{r xgboost estimated performance, cache=TRUE}
res_knn_none <- readRDS("./data/ModelCache/knn_none_xgboost_res.RDS")
res_knn_corr <- readRDS("./data/ModelCache/knn_corr_xgboost_res.RDS")
res_knn_pca <- readRDS("./data/ModelCache/knn_pca_xgboost_res.RDS")
res_meanmode_none <- readRDS("./data/ModelCache/MeanMode_none_xgboost_res.RDS")
res_meanmode_corr <- readRDS("./data/ModelCache/MeanMode_corr_xgboost_res.RDS")
res_meanmode_pca <- readRDS("./data/ModelCache/MeanMode_pca_xgboost_res.RDS")

# res_knn_none <- readRDS("H:/BIOS_6720/Project_YL/knn_none_xgboost_res.RDS")
# res_knn_corr <- readRDS("H:/BIOS_6720/Project_YL/knn_corr_xgboost_res.RDS")
# res_knn_pca <- readRDS("H:/BIOS_6720/Project_YL/knn_pca_xgboost_res.RDS")
# res_meanmode_none <- readRDS("H:/BIOS_6720/Project_YL/MeanMode_none_xgboost_res.RDS")
# res_meanmode_corr <- readRDS("H:/BIOS_6720/Project_YL/MeanMode_corr_xgboost_res.RDS")
# res_meanmode_pca <- readRDS("H:/BIOS_6720/Project_YL/MeanMode_pca_xgboost_res.RDS")

res <- c(KNN_none = res_knn_none,
         KNN_corr = res_knn_corr,
         KNN_pca = res_knn_pca,
         MeanMode_none = res_meanmode_none,
         MeanMode_corr = res_meanmode_corr,
         MeanMode_pca = res_meanmode_pca
)
res_sum <- summary(res)
res_mean_sum <- cbind(res_sum[, , Metric = "Brier"][, 1],
      res_sum[, , Metric = "Accuracy"][, 1],
      res_sum[, , Metric = "Kappa"][, 1],
      res_sum[, , Metric = "ROC AUC"][, 1],
      res_sum[, , Metric = "Sensitivity"][, 1],
      res_sum[, , Metric = "Specificity"][, 1])
colnames(res_mean_sum) <- c("Brier", "Accuracy", "Kappa", "ROC AUC",
                            "Sensitivity", "Specificity")
kable(res_mean_sum)
```

* All models are not doing well for specificity.
* KNN_corr appears to do better among these models for its highest accuracy, highest kappa, reasonable brier, roc auc, sensitivity and specificity.
* p-values are not significant.

## Estimated Performance II

```{r xgboost estiamted performance plot}
plot(res)
```

```{r xgboost estiamted performance t.test, echo = FALSE}
t.test(diff(res))
```

## Calibration Plots

```{r}
knn_corr_fit <- readRDS("./data/ModelCache/MeanMode_corr_xgboost_fit.RDS")
# knn_corr_fit <- readRDS("H:/BIOS_6720/Project_YL/knn_corr_xgboost_fit.RDS")

# ## In-sample calibration
# obs <- response(knn_corr_fit)
# pred <- predict(knn_corr_fit)
# cal_in <- calibration(obs, pred)
# plot(cal_in, se = TRUE)

## Out-of-sample calibration (resample estimation)
cal_out <- calibration(res_knn_corr)
plot(cal_out, se = TRUE)

## Smoothed curve
cal <- calibration(res_knn_corr, breaks = NULL)
plot(cal, se = TRUE)
```

## Variable Importance

```{r, cache=TRUE}
# variable importance
set.seed(123123)
varimp(knn_corr_fit) %>% plot(n = 5)
```

## Partial Dependence

```{r}
pd <- dependence(knn_corr_fit, select = c(alphaFreto))
plot(pd)
```

# Support Vector Machine Model

## Description

```{r, cache = TRUE}
fit_jt <- readRDS("./data/ModelCache/knn_none_SVMRad_fit.RDS")
sfit <- summary(as.MLModel(fit_jt))
hyperparam <- Reduce(cbind, sfit$TrainingStep1$params[sfit$TrainingStep1$selected,])
```


* A support vector machine attempts to draw a non-linear boundary to classify the outcome by transforming the predictors using basis functions

* In this transformed space, find linear boundaries using support vector classifier.

* We just need to know the kernel function on the basis functions, don't need to know basis function itself. 

* Radial Basis kernel function: $\exp(-\sigma^2\Vert x - x' \Vert^2)$; $\sigma =$ `r hyperparam$sigma`

* Cost of misclassification C =`r hyperparam$C`

* imputation using KNN with neighbors = 4


## Estimated Performance

```{r, cache = TRUE}
resamp_jt <- readRDS("./data/ModelCache/knn_none_SVMRad_res.RDS")
sres_jt <- as.data.frame(summary(resamp_jt))
sres_jt_df <- as.data.frame(sres_jt) %>% 
  pivot_wider(names_from = Statistic, values_from = Value) 

sres_jt_df %>%
  kable(caption = "SVMRad results with knn imputation",
        digits = 3) %>%
  kable_styling(full_width = FALSE)
```


## Calibration Plots

```{r out.height="70%", out.width="70%", fig.align='center', results='hide'}
plot(calibration(resamp_jt), se = TRUE)
```

* Decently calibrated. Low probabilities have many false negatives.

## Variable Importance

```{r warning=FALSE, out.width="80%", out.height="80%", fig.align='center', cache = TRUE}
set.seed(123123)
if(file.exists("./vi_cache/knn_none_SVMRad_vi.RDS")){
  vi_jt <- readRDS("./vi_cache/knn_none_SVMRad_vi.RDS")
}else{
  vi_jt <- varimp(fit_jt)
  saveRDS(vi_jt, "./vi_cache/knn_none_SVMRad_vi.RDS")
}
plot(vi_jt, n=15)
```

## Variable Importance

\begin{itemize}
  \item { Symptoms (performance)}

  \item { Indicators of liver injury/disease/infection (hepBsurfAnti, hepCvirAnti,aspartateTransaminase, alkalinePhosphatase, ferritin, totalProteins, ascites, hemoglobin)}

  \item { Biological Characteristics (age, portalVeinThromb)}

  \item { Risk factors (diabetes, artHyper)}
  
  \item {behavioral/demographic (endemicCountries, cigPerYr)}
\end{itemize}


## Partial Dependence: diabetes

```{r results='hide'}
pd <- dependence(fit_jt, select = c(diabetes))
plot(pd)
```

## Partial Dependence: ferritin

```{r results='hide'}
pd <- dependence(fit_jt, select = c(ferritin))
plot(pd)
```


## Partial Dependence ascites

```{r results='hide'}
pd <- dependence(fit_jt, select = c(ascites))
plot(pd)
```

## Partial Dependence: artHyper

```{r results='hide'}
pd <- dependence(fit_jt, select = c(hepCvirAnti))
plot(pd)
```


## Partial Dependence: symptom

```{r, warning=FALSE, results='hide'}
pd <- dependence(fit_jt, select = c(artHyper))
plot(pd)
```

# Final Model choice

## Criteria

* Final model was chosen by the model with the highest Sensitivity

* This corresponds to correctly identifying surviving patients (wrt cutoff = 0.5)

* Minimizes incorrectly telling a patient they will die

```{r}
res_yl <- summary(res_meanmode_corr)
sres_yl_df <- as.data.frame(res_yl) %>% 
  pivot_wider(names_from = Statistic, values_from = Value)

all_results <- rbind(sres_sl_df %>% 
                       filter(Metric == 'Sensitivity'),
                     sres_yl_df %>% 
                       filter(Metric == 'Sensitivity'),
                     sres_jt_df %>% 
                       filter(Metric == 'Sensitivity')
                     )

```


# References {.allowframebreaks}